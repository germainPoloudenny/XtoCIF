\section{Introduction}

Characterizing the atomic structure in functional materials is fundamental for understanding and optimizing materials for e.g. new energy technologies. Such characterization can be done using X-ray diffraction (XRD), and in particular, powder X-ray diffraction (PXRD) is widely used in materials chemistry and related fields as a main characterization tool~\citep{Cheetham2014}. Several advances in machine learning (ML) have shown promise in aiding analysis of diffraction data over the last decade~\citep{Tatlier2011, Bunn2016, Oviedo2019, Wang2020}, including recent advances in generative models for crystal structure prediction (CSP)~\citep{jiao2023crystal, mohanty2024crystext, antunes2024crystalstructuregenerationautoregressive}. These conventional ML-aided CSP methods typically explore the structural phase space guided by high-level descriptors like material composition. Emerging \textit{experimental data informed CSP} approaches have started to integrate diffraction data directly into the generative process~\citep{kjaer2023deepstruc, guo2024abinitiostructuresolutions, riesel2024crystal, lai2025endtoendcrystalstructureprediction}. This shift represents a fundamental departure from conventional CSP.

\begin{figure}[t]
\begin{center}
\begin{minipage}{0.48\textwidth}
\centering
\vspace{0.1in}
\includegraphics[width=\textwidth]{Figures/illustrations/deCIFer_overview.png}
\vspace{0.1in}
\subcaption{Overview of the deCIFer model.}
\label{fig:decifer_overview_subfig}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\includegraphics[width=\textwidth]{Figures/generated_examples/generated_samples_hero.png}
\subcaption{Example generations using deCIFer.}
\label{fig:generated_samples_subfig}
\end{minipage}
\caption{(a) Overview of the deCIFer model, which performs autoregressive crystal structure prediction (CSP) from PXRD data, optionally guided by tokenized crystal descriptors. PXRD embeddings are prepended to the CIF token sequence, enabling the generation of structurally consistent CIFs directly from diffraction data. (b) Three examples from the NOMA test set showing deCIFer generations, each illustrating a reference structure, the generated structure and their corresponding PXRD profiles.}
\label{fig:combined_decifer_examples}
\end{center}
\vskip -0.2in
\end{figure}

In this work, we present deCIFer (Figure~\ref{fig:decifer_overview_subfig}), a transformer-based model that performs CSP using the rich structural information explicitly represented in experimental data, such as PXRD patterns. Our autoregressive language model directly generates the Crystallographic Information Files (CIFs) -- text files that encode for crystal structures. This approach can significantly advance the workflow used for atomic structure characterization based on PXRD methods. Furthermore, we mimic the noise in the PXRD as a result of experimental variations using various transformations to the PXRD patterns.

The performance of deCIFer is evaluated on a diverse set of PXRD patterns, demonstrating its robustness to experimental noise and its ability to generate high-quality crystal structures. 
% By using learned conditioning embeddings to extract fine-grained structural features from PXRD patterns, 
deCIFer is the first experimental data informed CSP model capable of generating syntactically correct and structurally meaningful CIFs that accurately reproduce reference PXRD patterns.

\noindent \\
Our key contributions in this work are:
\begin{enumerate}[leftmargin=.5cm]    
\itemsep0em 
    \item Integration of experimental PXRD conditioning into an autoregressive transformer for CSP in CIF format; a capability not demonstrated in existing transformer-based generative models.
    \item Effective conditioning mechanism for autoregressive models with variable length input.
    \item Simulation of realistic experimental settings for PXRD with Gaussian noise and peak broadening. 
    \item Comprehensive evaluation on two large-scale datasets: NOMA\footnote{NOMA stands for \textbf{N}OMAD~\citep{nomad2019} \textbf{O}QMD~\citep{kirklin2015open} \& \textbf{M}P~\citep{materialsproject2013} \textbf{A}ggregation.
    }
    \& CHILI-100K~\citep{FriisJensenJohansen2024}, and comparison to state-of-the-art CSP generative models.
    
\end{enumerate}

\section{Background and Related Work} 

{\bf PXRD}: It is arguably the most accessible structural probe in solid-state chemistry. Modern benchtop diffractometers can collect a high-quality scan within minutes and are available in most research and industrial laboratories. 
A PXRD pattern contains diffraction peaks, whose position and intensity contain information on the periodic structure in crystalline materials, i.e. their atomic positions and structure symmetry. Crucially, the forward simulation of PXRD patterns from CIFs is grounded in well-established scattering theory~\citep{west2014solid}, enabling realistic modelling.


Most quantitative analysis of PXRD data is done using structure refinement, where the parameters in a structural model is refined against experimental data~\citep{young1995rietveld}. This requires a structural starting model of the correct structure type and symmetry. Identifying a model for such analysis is often referred to as \textit{fingerprinting}, and can be a very challenging task, where chemical intuition as well as extensive database searches are required. Even then, model identification is not always successful, and this hinders further material development. 

{\bf CSP with LLMs}: Transformer architecture~\citep{vaswani2017attentionneed} based large language models (LLMs) have seen emerging use in the automation of chemical syntheses~\citep{hocky2022natural, szymanski2023autonomous, m2024augmenting}, data extraction~\citep{gupta2022matscibert, dagdelen2024structured, polak2024extracting, schilling2025text}, and materials simulation and property prediction~\citep{zhang_dpa-2_2024, rubungo2024llmmatbench, jablonka2024leveraging}. However, they are not yet as widely used in tasks like materials design. ChemCrow~\citep{m2024augmenting}, for instance, is an LLM-powered chemistry search engine designed to automate reasoning tasks in materials design and other domains.

Recent works have used fine-tuning to adapt LLMs for CSP. \citet{gruver2024finetuned} fine-tuned Llama-2 models~\citep{touvron2023llama} on text-encoded atomistic data, enabling tasks like unconditional generation of stable materials. Similarly, \citet{mohanty2024crystext} fine-tuned LLaMA-3.1-8B~\citep{dubey2024llama} using QLoRA~\citep{dettmers2024qlora} for efficient CIF generation conditioned on material composition and space group. In contrast, the work on CrystaLLM~\citep{antunes2024crystalstructuregenerationautoregressive} relies on pre-training alone to generate CIFs. It is trained on an extensive corpus of CIFs representing millions of inorganic compounds. 

CrystaLLM relies on composition- and symmetry- level crystal descriptors. Its utility, as most other CSP methods, is thus to generate structures without direct reference to experimental data. deCIFer, on the other hand, conditions on PXRD signals, produces structures agreeing with observed diffraction patterns and address the need for models that bridge predictive power with experimental reality.

{\bf CSP with diffusion models}: In parallel to LLM-based approaches, generative models using diffusion- or flow-based frameworks have also emerged for CSP~\citep{jiao2023crystal, millerflowmm, zeni2025generative, xie2022crystaldiffusionvariationalautoencoder}. These methods rely on composition or partial structural constraints to guide generation, which requires domain knowledge and can limit direct incorporation of experimental data. While they show promise in producing stable crystal configurations, the gap between purely computational crystal structures and experimental data remains a challenge. The recent diffusion-based framework MatterGen~\citep{zeni2025generative} addresses some of these challenges by enabling conditioning on a broad range of property constraints, and it can generate structures that are more likely to be synthesizable than previous methods; nevertheless, conditioning on PXRD has not been demonstrated in its current implementation, leaving room for further exploration in integrating experimental data more directly.

\section{Methods}

Consider a crystal structure in CIF (text input) format that is tokenized into a sequence of length $T_i$: $\xbf^i = (x^i_1,x^i_2, \dots, x^i_{T_i})$ (see Appendix~\ref{sup-sec:cif_tokenization} for details on tokenization). The corresponding scattering pattern, which in our case is the PXRD pattern denoted by $\ybf^i$, is a continuous-valued vector representing the intensity profile of the scattering pattern. The dataset then consists of pairs of CIFs and PXRD patterns, $\Dcal = [(\xbf^i,\ybf^i)]_{i=1}^{N}$. Given this dataset, we want to minimize the negative conditional log-likelihood over the training data: 
%
\begin{equation}
    \Lcal(\Xbf|\Ybf;\Thetabf) = \frac{1}{N} \sum_{i=1}^{N} \left( -\sum_{t=1}^{T_i} \log P_\Thetabf(x^i_t|x^i_{<t},\ybf^i)\right). 
\end{equation}
This is achieved using a conditional autoregressive model $f_\Thetabf(\cdot)$ with trainable parameters $\Thetabf$ based on the transformer architecture~\citep{vaswani2017attentionneed}. Our autoregressive model, deCIFer, generates structures in CIF format when conditioned with an initial prompt and the PXRD data.

\paragraph{PXRD Conditioning:} 
The PXRD data can be seen as the structural fingerprint of the structures in CIFs. We use this PXRD data to steer the CSP by using it as a conditioning input to our model. 

Following the standard procedure in materials chemistry, for each CIF, we generate the discrete diffraction peak data, given as the set $\Pcal = \{(q_k,i_k)\}_{k=1}^n$ using \texttt{\small pymatgen}~\citep{Ong2013}. During the model training, $\Pcal$ is transformed into the PXRD data, $\ybf$, using different simulated experimental conditions. Formally, let $\Tcal$ be a set of transformations that can be applied to each $\Pcal$.

The family of transformations used in this work are designed to closely mimic the experimental conditions. 
We define a distribution of transformations $\Tcal$ such that each $\tau \sim \Tcal$ comprises 1) a random peak broadening with full width at half maximum (FWHM) $\sim\mathcal{U}(0.001, 0.100)$ and 2) additive noise with variance $\sigma_{\mathrm{noise}}^2\sim\mathcal{U}(0.001, 0.050)$. Concretely, each $\tau$ is the composition of these two steps, and new values for broadening and noise are sampled on each draw, $\ybf = \tau(\Pcal)$. Hence for each CIF, the PXRD pattern is transformed slightly differently every time it appears during training. Later for evaluation, we use $\tau_{\mathrm{fixed}}$ with manually specified parameters to test the robustness of the models. The \textit{clean} transformation $\tau_0$ fixes FWHM = $0.05$ and $\sigma_{\mathrm{noise}}^2=0$. Examples from $\Tcal$ on a PXRD are shown in Figure~\ref{noise_broadeness_levels} (in Appendix).

\paragraph{Conditioning Model:} A multi-layered perceptron (MLP) $f_\Phibf(\ybf)$ with trainable parameters $\Phibf$ is used to embed the PXRD data into a learnt vector $\ebf = f_\Phibf(\ybf) \in \mathbb{R}^D$. This embedding is inserted at the start of the tokenized CIF sequence as a conditioning signal. Joint training of the conditioning network, $f_\Phibf$, and the autoregressive model, $f_\Thetabf$, enables the integration of structural information from the PXRD profiles during the generation of CIFs. This joint training along with transformations to the PXRD data results in the final training objective $\Lcal(\Xbf|\Ybf;\Thetabf,\Phibf)$.

\paragraph{Sequence Packing and Isolating CIFs:}  
To effectively train on variable-length CIF sequences, we implement a batching strategy during training, inspired by recent sequence-packing methods that prevent cross-contamination and improve throughput~\citep{KosecFuKrell}. Our approach concatenates multiple tokenized CIF sequences, each of length $T_i$ (excluding the conditioning embedding) into segments of fixed context length, $C$. Formally, consider a sequence $\mathbf{S} = [\mathbf{e}^{1}, \tbf^{1}_1, \dots, \tbf^{1}_{T_1}, \mathbf{e}^{2}, \tbf^{2}_1, \dots, \tbf^{n}_k]$, where $n$ is the last fully or partially included CIF. In this notation $\ebf^{i}$ denotes a $D$-dimensional conditioning embedding for the $i$-th CIF, while $\tbf_j^i$ is the $D$-dimensional input embedding for $x_j^i$, the $j$-th token of the $i$-th CIF. We choose $k$ such that $|\mathbf{S}| = C$. If adding another CIF exceeds $C$, that CIF is split at the boundary and continued in the next segment. In practice, we set $C=3076$ based on the available GPU memory to balance throughput and memory constraints, which exceeds the length of the longest tokenized CIFs in the NOMA dataset. Although our method guarantees efficient batch utilization, it does occasionally split exceptionally long CIFs between batches ($\approx\!0.04$\% of samples in the NOMA dataset; see Figure~\ref{sup-fig:dataset_statistics_NOMA} in the Appendix). To mitigate this, we shuffle the training set at the start of each epoch so that previously split CIFs are more likely to appear in full in subsequent mini-batches, allowing the model to learn from complete sequences.

To isolate different CIFs in the same sequence, we employ an attention mask $\Mbf$, defined such that $M_{kl} = 1$ iff tokens $k$ and $l$ belong to the same CIF, and $M_{kl} = 0$ otherwise. This results in a block-wise diagonal, upper-triangular, attention matrix as shown in Figure~\ref{fig:attn_masking} (Appendix). To prevent positional information from leaking across CIF boundaries we also reset positional encodings at the start of each CIF by assigning positions from $0$ to $T_{i}-1$ for the $i$-th sequence. 

\section{Dataset and Experiments}

\paragraph{Dataset:} We use two large-scale public datasets here. The first, NOMA, is a large-scale compilation of crystal structures curated in CrystaLLM~\citep{antunes2024crystalstructuregenerationautoregressive}, which draws from the Materials Project (April 2022)~\citep{materialsproject2013}, OQMD (v. 1.5, October 2023)~\citep{kirklin2015open}, and NOMAD (April 2023)~\citep{nomad2019} databases. The second, CHILI-100K~\citep{FriisJensenJohansen2024}, is a large-scale dataset of experimentally determined crystal structures obtained from a curated subset of the Crystallography Open Database (COD)~\citep{COD2009}. We employ NOMA for both training and testing our models, whereas CHILI-100K is used {\em exclusively for testing}. Both datasets are open-source and available for download.\footnote{NOMA: \url{github.com/lantunes/CrystaLLM} (CC-BY 4.0 licence), CHILI-100K: \url{github.com/UlrikFriisJensen/CHILI} (Apache 2.0 licence).}

\paragraph{Preprocessing:} We perform standard data preprocessing steps outlined in CrystaLLM before performing additional curation to address complexities such as ionic charges and to ensure consistent structural representation between NOMA and CHILI-100K. The resulting NOMA dataset comprises approximately $2.3$M CIFs, spanning $1$--$10$ elements, including elements up to atomic number $94$ (excluding polonium, astatine, radon, francium, and radium). Duplicate structures were filtered by selecting the configuration with the lowest volume per formula unit. All structures were converted to a standardized CIF format using the \texttt{\small pymatgen} library~\citep{Ong2013} and all floating point values were rounded to four decimal places. The resulting CHILI-100K dataset consists of $\approx\!8.2$K CIFs, spanning $1$--$8$ elements, including elements up to atomic number $85$. Figures~\ref{sup-fig:dataset_statistics_NOMA} and~\ref{sup-fig:dataset_statistics_CHILI_100K} (in Appendix) show the distribution of elemental compositions, space groups, and more attributes for the two datasets. 

Additionaly, a known challenge in datasets like NOMA is the disproportionate representation of high-symmetry structures, which can skew model learning and evaluation~\citep{Davariashtiyani_2024, Zhang2023}. To mitigate this, we stratify the dataset during the train/validation/test split based on space group labels. For details on stratification, see Section~\ref{sup-sec:model_architecture}.

Finally, CIFs from both datasets were tokenized into a vocabulary of $373$ tokens, encompassing CIF tags, space group symbols, element symbols, numeric digits, punctuation marks, padding tokens and a special conditioning token. For a detailed explanation of the standardization and tokenization processes, refer to Section~\ref{sup-sec:cif_standardization} and \ref{sup-sec:cif_tokenization} in the Appendix.

\paragraph{Model Hyperparameters:} deCIFer has two learnable components, $f_{\Phibf}$ and $f_{\Thetabf}$. The encoder $f_{\Phibf}$ is a 2-layer MLP that takes a PXRD profile of dimension $1000$ and outputs a $512$-dimensional embedding. This embedding is prepended as the conditioning-token to the sequence of CIF tokens, where each CIF token is also of $D=512$ dimensions. $f_{\Thetabf}$ is a decoder-only transformer~\citep{vaswani2017attentionneed} with $8$ layers, each containing $8$ attention heads. We use a context length of $3076$ and a batch size of $32$. A linear warm-up of the learning rate is applied over the first $100$ epochs, followed by a cosine decay schedule using AdamW~\citep{AdamW2017} ($\beta_1 = 0.9, \beta_2 = 0.95$, weight decay $10^{-1}$) for $50$K epochs with gradient accumulation of $40$ steps on a single NVIDIA A100 GPU with mixed-precision acceleration. All aspects of the model architecture were implemented in Pytorch~\citep{Pytorch2019}. $f_{\Phibf}$ has $\approx\!0.78$M trainable parameters and $f_{\Thetabf}$ contains $\approx\!26.94$M, resulting in a deCIFer model with $27.72$M parameters. The code is open-source and additional details can be found in Section~\ref{sup-sec:model_architecture} in the Appendix. 

\paragraph{Evaluation:}

Figure~\ref{fig:evaluation_pipeline} summarizes the evaluation pipeline which shows how a reference CIF from the test set is used to generate discrete peaks, $\mathcal{P} = \{(q_k, i_k)\}_{k=1}^n$, which are transformed into a PXRD profile $\mathbf{y} = \tau_{\mathrm{fixed}}(\mathcal{P})$, where $\tau_{\mathrm{fixed}}$ is selected according to the desired experimental setting. The resulting PXRD, along with any optional crystal descriptors, is tokenized and passed to deCIFer to produce a new CIF ($\text{CIF}^*$). For evaluation, the generated structures are compared using three common metrics. Here a \emph{clean} reference transformation $\tau_0$ (FWHM$=0.05$, $\sigma_{\mathrm{noise}^2} = 0$) is used.

\begin{wrapfigure}{r}{0.5\textwidth}
\vspace{-0.75cm}
\begin{center}
\centering
\includegraphics[width=0.49\textwidth]{Figures/illustrations/evaluation_pipeline.png}
\vskip 0.1in
\caption{Evaluation pipeline: A test set CIF generates a PXRD profile, tokenized for deCIFer to produce a new CIF, compared to the reference using a clean transformation.}
\label{fig:evaluation_pipeline}
\end{center}
\vspace{-0.75cm}
\end{wrapfigure}
1) {\bf Residual weighted profile} ($R_{\mathrm{wp}}$) is applied to convolved continuous PXRD profiles, measuring the discrepancy between a reference PXRD profile, $\ybf$, and a generated PXRD profile, $\ybf^*$. Formally:
        $R_\mathrm{wp} = \sqrt{\frac{\sum_{i} w_i (y_i - y_i^*)^2}{\sum_{i} w_i y_i^2}}$ with all weights $w_i = 1$ here, following convention.

2) {\bf Match rate} (MR) uses \texttt{\small StructureMatcher}~\citep{Ong2013} to assess structural similarity between reference and generated CIFs. Two structures are considered a match if their lattice parameters, atomic coordinates, and space group symmetries are within the defined tolerances. MR is the fraction of matching structures. See Section~\ref{sup-sec:match_rate} in the Appendix for more information.

3) {\bf Validity} (Val.) ensures the internal consistency of each generated CIF by checking formula consistency, site multiplicity, bond lengths, and space group alignment. A CIF is deemed \textit{overall valid} only if it passes all four checks. Detailed explanations of these metrics are provided in Section~\ref{sup-sec:validity} in the Appendix.

\paragraph{Experiments:} We conducted a series of experiments to evaluate deCIFer's ability to perform CSP when conditioned on PXRD profiles, space group, and composition. First, we compare against three recent state-of-the-art CSP models. Next, we compare the baseline performance of deCIFer against an unconditioned variant (U-deCIFer) to assess the direct impact of PXRD conditioning on the generated structures. Next, we introduce controlled noise and peak broadening into the input PXRD data, examining deCIFer's robustness under more challenging scenarios resembling real-world PXRD data. Finally, we apply deCIFer (trained on NOMA) to CHILI-100K~\citep{FriisJensenJohansen2024} to demonstrate its scalability and performance on more complex crystal systems. In these experiments, we generate one CIF for each reference sample in the test sets. For consistent evaluation, each reference CIF is also processed through the fixed, \textit{clean} transformation $\tau_0$ (FWHM $= 0.05$, $\sigma_{\mathrm{noise}}^2 = 0$).

For some experiments we allow space group and composition to be specified as crystal descriptors. Since CIFs inherently encode space group and composition as text, we treat these descriptors as standard tokens in our vocabulary (see Section~\ref{sup-sec:cif_tokenization} in the Appendix). The space group appears in the CIF header, while composition is stored as element–count pairs. During inference, these tokens are optionally inserted into the CIF.

\section{Results}
\label{sec:results}

\paragraph{Baseline Comparisons with State-of-the-art:} 
To evaluate the structure generation quality of deCIFer, we compare against three recent state-of-the-art CSP models: CDVAE~\citep{xie2022crystaldiffusionvariationalautoencoder}, DiffCSP~\citep{jiao2023crystal}, and CrystaLLM~\citep{antunes2024crystalstructuregenerationautoregressive}. Following the single-sample evaluation setup used in prior work~\citep{antunes2024crystalstructuregenerationautoregressive}, we generate one structure per composition and assess accuracy using two metrics: match rate and root-mean-square error (RMSE) in atomic positions (Å). To ensure a fair and consistent evaluation, we adopt the official train-test splits provided by DiffCSP~\citep{jiao2023crystal} for all four benchmark datasets: Perov-5~\citep{Castelli2012, Castelli2012_2}, Carbon-24~\citep{Pickard2020CarbonData}, MP-20~\citep{Jain2013MaterialsProject}, and MPTS-52~\citep{Baird2023MPTS}.


As shown in Table~\ref{tab:baselines}, deCIFer outperforms all baselines on Perov-5 and Carbon-24, highlighting the benefit of PXRD conditioning when the diffraction data are informative. However, on MP-20 and MPTS-52, CrystaLLM-large outperforms deCIFer, likely due to its exclusive emphasis on composition-structure mappings, which align closely with the test set. This highlights a key trade-off in experimental data informed CSP: while conditioning can enhance structure generation in complex cases, it can also introduce ambiguity when the PXRD signal is weak or the model’s training priors are strong.

\begin{table*}[ht]
\centering
\scriptsize
\caption{Performance comparison on four public CSP benchmarks: Perov-5~\citep{Castelli2012, Castelli2012_2}, Carbon-24~\citep{Pickard2020CarbonData}, MP-20~\citep{Jain2013MaterialsProject}, and MPTS-52~\citep{Baird2023MPTS}.  
Following the single-sample evaluation protocol used in prior work~\citep{antunes2024crystalstructuregenerationautoregressive}, one structure is generated per test composition.  
We report the match rate (\%) based on structural equivalence under \texttt{StructureMatcher} and the atomic root-mean-square error (RMSE in \AA) after alignment.}
\label{tab:baselines}
\begin{tabular}{lcccccccc}
\toprule
 & \multicolumn{2}{c}{\textbf{Perov-5}} & \multicolumn{2}{c}{\textbf{Carbon-24}}
 & \multicolumn{2}{c}{\textbf{MP-20}} & \multicolumn{2}{c}{\textbf{MPTS-52}}\\
\cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-9}
Model & Match~(\%)$\uparrow$ & RMSE$\downarrow$ &
Match~(\%)$\uparrow$ & RMSE$\downarrow$ &
Match~(\%)$\uparrow$ & RMSE$\downarrow$ &
Match~(\%)$\uparrow$ & RMSE$\downarrow$\\
\midrule
CDVAE            & 45.31 & 0.1138 & 17.09 & 0.2969 & 33.90 & 0.1045 &  5.34 & 0.2106\\
DiffCSP          & 52.02 & 0.0760 & 17.54 & 0.2759 & 51.49 & 0.0631 & 12.19 & 0.1786\\
CrystaLLM-small  & 47.95 & 0.0966 & 21.13 & 0.1687 & 55.85 & 0.0437 & 17.47 & 0.1113\\
CrystaLLM-large  & 46.10 & 0.0953 & 20.25 & 0.1761 & 58.70 & 0.0408 & 19.21 & 0.1110\\
U-deCIFer        & 50.55 & 0.1177 & 17.33 & 0.1526 & 44.99 & 0.0784 & 11.80 & 0.1563\\
\midrule
\textbf{deCIFer} & 85.29 & 0.0491 & 37.16 & 0.1970 & 43.51 & 0.0763 & 11.44 & 0.1346\\
\bottomrule
\end{tabular}
\vspace{-0.3cm}
\end{table*}

\paragraph{Importance of PXRD Conditioning:}

We compare deCIFer and U-deCIFer to study the effect of PXRD conditioning. We evaluate each model in three settings: without any crystal descriptors (``none''), with only compositional information (``comp.''), and with both compositional and space group information (``comp. + s.g.''). Both models share the same architectural backbone, but deCIFer receives structural guidance via the PXRD conditioning. 

Figure~\ref{fig:combined_violin_table_baseline} shows that deCIFer achieves lower average $R_{\mathrm{wp}}$ than U-deCIFer across all descriptor settings. Figure~\ref{fig:combined_violin_table_baseline} visualizes the distributions of $R_{\mathrm{wp}}$ for both models, highlighting the large performance improvement due to PXRD conditioning, and a smaller but still notable improvement due to composition conditioning. Figure~\ref{fig:combined_crystal_systems_gen_samples} shows that structures of more common crystal systems lead to better outcomes, while under-represented, lower symmetry crystal systems remain challenging. The figure further illustrates the effectiveness of PXRD conditioning with three examples from the NOMA test set, showcasing the range of generated structures, from near-perfect alignment to structural mismatch. While U-deCIFer benefits from the addition of crystal descriptors, it never reaches the accuracy of deCIFer. 
Overall, these results demonstrate that conditioning on PXRD data and crystal descriptors substantially enhances the model's ability to generate CIFs with close alignment to desired PXRD profiles.

\begin{figure}[t]
\begin{center}
\begin{minipage}{0.4\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{Figures/baseline/baseline_violin_plot_metrics.pdf}
\end{minipage}
\begin{minipage}{0.55\textwidth}
\centering
\scriptsize
\setlength{\tabcolsep}{4pt}
\begin{tabular}{llccc}
\toprule
{\bf Desc.} & Model & $R_{wp}$ { $(\mu \pm \sigma) \downarrow$} & Val. (\%) $\uparrow$ & MR (\%) $\uparrow$ \\
\midrule
\multirow{2}{*}{\bf none} & U-deCIFer & $1.24$ {$\pm 0.26$} & 93.49 & 0.00 \\
                          & deCIFer   & $0.32$ {$\pm 0.34$} & 92.66 & 5.01 \\
\midrule
\multirow{2}{*}{\bf comp.} & U-deCIFer & $0.82$ {$\pm 0.41$} & 93.78 & 49.30 \\
                          & deCIFer   & $0.25$ {$\pm 0.29$} & 93.73 & 91.50 \\
\midrule
\multirow{2}{*}{\bf comp.+s.g.} & U-deCIFer & $0.65$ {$\pm 0.36$} & 93.72 & 87.07 \\
                                & deCIFer   & $0.24$ {$\pm 0.29$} & 93.90 & 94.53 \\
\bottomrule
\end{tabular}
\end{minipage}

\caption{Left: Distribution of $R_{\mathrm{wp}}$ for deCIFer and U-deCIFer on the NOMA test set with boxplots. Lower $R_{\mathrm{wp}}$ indicates better CIF alignment. Right: Performance for 20K NOMA test samples using deCIFer and U-deCIFer with different descriptors: \textbf{none} (no descriptors), \textbf{comp.} (composition), and \textbf{comp.+ s.g.} (composition + space group). Metrics include validity (Val.) and match rate (MR).}
\label{fig:combined_violin_table_baseline}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[t]
\begin{center}
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Figures/baseline/baseline_crystal_systems_metrics.pdf}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\centering
\includegraphics[width=0.9\textwidth]{Figures/generated_examples/generated_samples_threeeaxmples.png}
\end{minipage}
\caption{Left: Average $R_{\mathrm{wp}}$ by crystal system for deCIFer on the NOMA test set shows better performance for common high symmetry systems and higher $R_{\mathrm{wp}}$ for rare low symmetry systems. Right: Examples from the NOMA test set highlight this trend with predicted structures from PXRD and composition maintaining reasonable matches even for low symmetry systems with higher $R_{\mathrm{wp}}$.}
\label{fig:combined_crystal_systems_gen_samples}
\end{center}
\vskip -0.2in
\end{figure}

\paragraph{Robustness to Perturbations in PXRD Conditioning}

We evaluated deCIFer's ability to generate accurate CIFs under varying levels of additive noise and peak broadening in the PXRD conditioning input, while also providing the composition as a crystal descriptor. Building upon the baseline scenario of clean, noise-free PXRD data, we tested several increasingly challenging conditions: maximum noise, maximum peak broadening, combined noise and broadening, and out-of-distribution (OOD) broadening levels beyond the model's training range.

Figure~\ref{fig:violin_table_robustness} 
(and Figure~\ref{sup-fig:violinplot_robustness} in Appendix) show that while additive noise results in slight performance degradation, the model remains robust to in-distribution noise and peak broadening. We also observe moderate performance degradation with OOD broadening or significantly higher noise levels. Unsurprisingly, we observe that lower-symmetry crystal systems remain more challenging to predict under perturbed conditions across all crystal systems. This is depicted in Figure~\ref{fig:barplot_robustness} in the Appendix.

\paragraph{OOD Evaluation on CHILI-100K}

To evaluate the generalization of deCIFer to more complex crystal systems, we tested its performance on the CHILI-100K dataset~\citep{FriisJensenJohansen2024} which had {\em no overlap with the NOMA training data} for deCIFer. Unlike synthetic datasets, CHILI-100K presents a closer approximation to real-world challenges, including complex structural motifs and a broader distribution of crystal symmetries. Additionally, CHILI-100K contains a significantly higher proportion of lower-symmetry structures compared to synthetic datasets like NOMA (see the sample distribution in Appendix Figure~\ref{fig:barplot_chili}). 


The results on CHILI-100k dataset are summarized in Figure~\ref{fig:violin_table_robustness}. deCIFer maintains a reasonable level of structural accuracy on this challenging dataset. The relatively low validity score is partly due to challenges with bond length validity, which was notably lower than the other validity metrics. A full breakdown of the validity metrics for this experiment is available in Section~\ref{sup-sec:validity} in the Appendix.

Despite the performance drop, deCIFer demonstrated robustness to added noise and peak broadening in PXRD inputs, with stable $R_{\mathrm{wp}}$ values across perturbed conditions as seen in Figure~\ref{fig:violin_table_robustness}. This stability, along with its ability to generate a variety of structural features, suggests that deCIFer could be useful for practical applications involving experimentally derived PXRD data.

\begin{wraptable}{r}{0.45\textwidth}
\vspace{-0.15cm}
\centering
\scriptsize
\caption{Performance on CHILI-100K without (U-deCIFer) and with (deCIFer) PXRD conditioning.
\label{tab:chili_ablate}}
\begin{tabular}{lccc}
\toprule
Model & $R_{\mathrm{wp}}\downarrow$ & Val.~(\%)$\uparrow$ & MR~(\%)$\uparrow$\\
\midrule
U-deCIFer & $0.96\pm0.32$  & $43.26$ & $25.92$\\
deCIFer    & $0.70\pm0.37$ & $41.83$ & $37.34$\\
\bottomrule
\end{tabular}
\vspace{-0.25cm}
\end{wraptable}

\begin{figure}[t]
\begin{center}
\begin{minipage}{0.4\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{Figures/chili100k/chili100k_violin_plot_metrics.pdf}
\end{minipage}
% \hfill
\begin{minipage}{0.55\textwidth}
\centering
\scriptsize  % Adjust font size for the table
\setlength{\tabcolsep}{3pt}
\begin{tabular}{lcccc}
\toprule
 {\bf Dataset} & ($\sigma_{\mathrm{noise}}^2$, {FWHM}) & $R_{\mathrm{wp}}\;(\mu \pm \sigma)\downarrow$ & Val. (\%)$\;\uparrow$ & MR (\%)$\;\uparrow$\\
\midrule
\multirow{3}{*}{\bf NOMA} & ID: (0.00, 0.05)
& 0.25$\pm$0.29 & 93.73 & 91.50\\
 & ID: (0.05, 0.10)
& 0.31$\pm$0.30 & 93.77 & 89.28\\
\cmidrule{2-5}
 & OOD: (0.10, 0.20)
& 0.65$\pm$0.34 & 91.66 & 77.66\\
\midrule
\multirow{3}{*}{\bf CHILI-100K} &  ID: (0.00, 0.05)
& 0.70$\pm$0.37 & 41.83 & 37.34 \\
&  ID: (0.05, 0.10)
& 0.73$\pm$0.36 & 40.95 & 35.97\\
\cmidrule{2-5}
 & OOD: (0.10, 0.20)
& 0.87$\pm$0.33 & 33.62& 26.09\\
\bottomrule
\end{tabular}
\end{minipage}
\vspace{-0.25cm}
\caption{Left: Distribution of $R_{\mathrm{wp}}$ for deCIFer on NOMA and CHILI-100K using PXRD conditioning and composition (\textbf{comp.}) across three scenarios: clean, high noise/broadening, and out-of-distribution (OOD), with noise and FWHM values indicated. Right: Corresponding table of performance metrics, including $R_{\mathrm{wp}}$, overall validity (Val.), and match rate (MR) for each scenario.}
\label{fig:violin_table_robustness}
\end{center}
\vspace{-0.5cm}
\end{figure}

\noindent \\
To assess the impact of PXRD conditioning on OOD data, we compare deCIFer with U-deCIFer on the CHILI-100K test set. Results are shown in Table~\ref{tab:chili_ablate}. We observe that PXRD conditioning lowers $R_{\mathrm{wp}}$ and has a significant positive effect on the match rate, showcasing that the conditioning drives improvement and not merely compositional priors.

\section{Discussion and Outlook}\label{sec:discussion}

{\bf PXRD-driven Structure Generation:} The experiments on NOMA and CHILI-100K in Section~\ref{sec:results} clearly show the utility of including conditioning data such as PXRD when performing CSP. deCIFer consistently achieves strong performance on the NOMA test set, where the diversity of crystal systems and the availability of high-quality PXRD signals allow the model to accurately generate structures that align with the diffraction data. This highlights a key advantage of PXRD conditioning: when the diffraction data are clear and informative, deCIFer can use them to resolve complex atomic arrangements, even in cases where composition-only models struggle.

However, the results also make it clear that deCIFer, like any PXRD-conditioned model, cannot be used as a black-box end-to-end solver. The model's ability to generate accurate structures is tied to the quality and relevance of the PXRD signal. On datasets like MP-20, where learned composition–structure priors dominate, PXRD conditioning can sometimes interfere with accurate generation by introducing conflicting guidance. This tension between learned priors and PXRD conditioning is not unique to deCIFer but reflects a broader challenge in PXRD-informed CSP. 

By embedding the PXRD data, $\ybf$, into a learnable conditional embedding, $\ebf = f_\Phibf(\ybf)$, and prepending it onto prompts for deCIFer, we have demonstrated a general paradigm for integrating additional conditional data in materials design, an idea which can be extended to the incorporation of other material properties. In cases where multiple experimental data sources are available, they can be easily injected into the generative process using specific conditioning models. That is, if $P$ properties are available such that $\ybf = \{\ybf_1,\dots,\ybf_P\}$, then deCIFer can be extended to incorporate these additional data using additional conditioning models, $f_{\Phibf_1},\dots,f_{\Phibf_P}$, that can be jointly trained with the generative model, $f_\Thetabf$, via a training objective of the form $\Lcal(\Xbf|\Ybf_1,\dots,\Ybf_P; \Thetabf, \Phibf_1,\dots,\Phibf_P)$.


{\bf Consistency in CIF generation:} To further investigate the consistency and variability of deCIFer, we generated 16K CIFs for the same PXRD profile with different crystal descriptors (``none'', ``comp.'', and ``comp+s.g.''). Figure~\ref{fig:self_consistency} illustrates our findings for a challenging monoclinic crystal system of Sr$_2$Cd$_2$Se$_4$ from the NOMA test set. When unconstrained by crystal descriptors, the model generates a wide diversity of cell-parameters, compositions, and space groups, yet the $R_{\mathrm{wp}}$ values tend to cluster in a relatively narrow range. In contrast, imposing compositional and, especially, space group constraints yields much tighter cell-parameter distributions and a broader $R_{\mathrm{wp}}$ distribution. The broader distributions highlight how the $R_{\mathrm{wp}}$ metric is highly sensitive to even small structural deviations and shows the importance of complementing with other validation methods. Despite these variations, the overall structural match rate to the reference CIF remains high. If the composition or space group is known with high confidence, incorporating these constraints can speed up convergence to a more accurate structural solution. Conversely, if the objective is to explore the material landscape more broadly, unconstrained generation with PXRD can be advantageous.

\begin{figure}[t]
\begin{center}
\centerline{\includegraphics[width=0.95\textwidth]{Figures/data_statistics/self_consistency_alt_version.png}}
\caption{deCIFer-sampled structures for a monoclinic Sr$_2$Cd$_2$Se$4$ PXRD profile (16K samples). a) Reference structure. b) Distribution of $R{\mathrm{wp}}$ for generated CIFs. c) Examples of generated structures showing best, median, and diverse samples. d) Distribution of sampled crystal systems. e) Histograms of cell lengths (a, b, c) and angles ($\alpha, \beta, \gamma$) with reference values as dotted lines.}
\label{fig:self_consistency}
\end{center}
\vspace{-1.05cm}
\end{figure}

{\bf Limitations}: 
In materials science, the concept of data leakage is nuanced due to the challenges in defining novelty and diversity, as emphasized in recent discussions on scaling AI for materials discovery~\citep{cheetham2024ai_materials_discovery}. Although we stratify the NOMA dataset by space group bins and use CHILI-100K exclusively for testing, there remains the possibility of implicit overlaps, such as structurally similar crystal entries or shared compositional biases. 

Furthermore, the vast size and diversity of the NOMA dataset as well as the thorough preprocessing steps (e.g., de-duplication, filtering, and standardization), coupled with the independent curation of CHILI-100K, significantly mitigate the impact of any potential overlaps. 

While PXRD conditioning enhances structure prediction by introducing rich physical constraints, it also inherits a well-known limitation: the inability to distinguish between homometric structures. These are distinct atomic arrangements that yield indistinguishable diffraction patterns~\citep{Patterson1944}. This ambiguity has been documented in real structural analyses~\citep{Schneider2010}. Because metrics such as $R_{\mathrm{wp}}$ operate only on the diffraction pattern, they cannot capture homometric degeneracy. Nevertheless, several strategies can help mitigate this limitation~\citep{Shen2022}. Particularly, our experiments with deCIFer show that even partial inclusion of complementary information, such as composition, can mitigate near-degeneracy cases where the only differences in the PXRD are subtle variations in relative peak intensities. 

\section{Conclusion}

In this work, we introduced deCIFer as a data-informed approach to CSP. deCIFer is an autoregressive transformer-based language model that generates CIFs directly from PXRD data developed using lab-scale compute resources with a single GPU. By conditioning on simulated PXRD profiles, deCIFer captures fine-grained structural information beyond what is possible with composition- or symmetry-based information alone. Evaluations on large synthetic and experimentally derived datasets highlight deCIFer's robust performance in scenarios with varying levels of noise and peak broadening.

Ultimately, deCIFer represents a step forward in PXRD-informed CSP, bridging the gap between purely composition-based generative models and structure generation directly guided by experimental data. It is most useful when the diffraction signal is informative and when combined with careful downstream verification. For real-world use, this balance must be clearly understood, and model predictions must be treated as high-quality starting points rather than definitive solutions.