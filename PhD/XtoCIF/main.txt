CVPR                                                                                                                                        CVPR
#XXXX                                                                                                                                       #XXXX
                             CVPR 2025 Submission #XXXX. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.




                            XtoCIF: Test-Time Scaling LLM for X-ray Diffraction

                                                    Anonymous CVPR submission

                                                             Paper ID XXXX


                                Abstract                                    ing strategy that conflates sequence generation with diffrac-    038
                                                                            tion agreement, and (iii) ad hoc evaluation of candidate         039
 001    Automated crystal structure prediction (CSP) can accel-             CIF portfolios when chemical composition or symmetry de-         040
 002    erate materials discovery by combining experimental data            scriptors are missing.                                           041
 003    with generative models. X TO CIF builds on DE CIF ER and                We introduce X TO CIF, a practitioner-oriented extension     042
 004    retains the original autoregressive architecture and tok-           that revisits training, decoding, and evaluation for PXRD-       043
 005    enizer. We introduce engineering improvements and a stan-           to-structure models. The framework accelerates data load-        044
 006    dardized powder X-ray diffraction (PXRD) evaluation pro-            ing and PXRD augmentation, exposes scriptable determin-          045
 007    tocol. The main changes are a GPU-optimized training                istic beam search alongside the default stochastic sam-          046
 008    pipeline with approximately 44× higher throughput, deter-           pler, and provides utilities that order candidate CIFs by        047
 009    ministic beam-search decoding, diffraction-aware rerank-            their post-hoc weighted-profile residual (Rwp ). Collec-         048
 010    ing using the weighted-profile residual Rwp , and a unified         tively, these changes enable systematic comparison across        049
 011    evaluation suite that covers descriptor-free, composition,          descriptor-free, composition-conditioned, and composition-       050
 012    and composition plus space-group conditioning.                      plus-space-group regimes without altering the underlying         051
 013        On NOMA, the descriptor-free setting benefits most:             autoregressive architecture or tokenizer.                        052
 014    Rwp decreases from 0.23 to 0.18 while the structural match              On the NOMA benchmark, the proposed training stack           053
 015    rate (MR) increases from 5.6 % to 8.1 %. Supplying compo-           delivers ∼ 44× higher throughput while maintaining fi-           054
 016    sition or composition plus space group keeps DE CIF ER ’s           delity to experimental artifacts, and the decoding proto-        055
 017    baseline near ≈ 97 % MR and reduces Rwp further. These              col improves both match rate (MR) and Rwp in low-                056
 018    gains result from training, inference, and evaluation engi-         information settings. For comparability, we retain the MR,       057
 019    neering rather than changes to the model, making X TO CIF           validity, and Rwp metrics established by DE CIF ER, use          058
 020    a practical approach to PXRD-to-structure generation that           third-person references to prior work, and report all abla-      059
 021    can be implemented concisely.                                       tions in the same evaluation harness.                            060


                                                                            Contributions. Relative to DE CIF ER, X TO CIF keeps the         061
 022    1. Introduction
                                                                            autoregressive generator and tokenizer unchanged. We con-        062
 023    Powder X-ray diffraction (PXRD) is the most widely avail-           tribute three self-contained, quantified components:             063
 024    able probe for characterizing crystalline intermediates, yet        • Training system (engineering). A GPU-optimized                 064
 025    turning a noisy one-dimensional scattering profile into an             pipeline that delivers ∼ 44× higher throughput on NOMA        065
 026    atomic structure still hinges on manual Rietveld-style re-             via streamlined data serving, efficient PXRD augmen-          066
 027    finement, curated priors, and hours of expert iteration [2,            tation, and multi-GPU orchestration. Single-GPU itera-        067
 028    10, 14]. Fully autonomous PXRD workflows therefore re-                 tion time drops from 257.9±4.2 s to 10.35±0.13 s, and         068
 029    quire models that can translate raw intensities into crystal-          to 5.82 ± 0.19 s on 2 GPUs (124 ± 2 and 220 ± 7 sam-          069
 030    lographic information files (CIFs) while remaining faithful            ples/s respectively), cutting a 300k-step run from ∼10        070
 031    to experimental noise and instrument effects.                          days to ∼15 hours (Table 1). This acceleration allowed        071
 032       Autoregressive language models conditioned on PXRD                  us to sweep dozens of random seeds and initialization         072
 033    embeddings (e.g., DE CIF ER [5]) have taken important steps            strategies, raising the composition-conditioned MR from       073
 034    in this direction by directly generating CIF tokens from               ≈ 94% to ≈ 97% without modifying the DE CIF ER back-          074
 035    diffraction traces. However, the baseline exposed three                bone (Figure 3).                                              075
 036    systemic bottlenecks: (i) multi-day training due to ineffi-         • Inference protocol. We expose deterministic beam               076
 037    cient data serving and augmentation, (ii) a single decod-              search alongside the stochastic sampler and make it easy      077


                                                                        1
CVPR                                                                                                                                                         CVPR
#XXXX                                                                                                                                                        #XXXX
                                CVPR 2025 Submission #XXXX. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.



                                                         PXRD Encoder                      dominated by iterative protocols such as Rietveld refine-          103
                                Augment & Normalize   1D CNN + Transformer
                                                                                           ment implemented in widely used suites like GSAS-II and            104
                                                                                           FullProf, simulated annealing, and charge-flipping searches        105
                                                                                           [10, 14]. These workflows assume accurate peak indexing,           106
         PXRD Trace (2θ, I)
                                                                                           carefully tuned peak-shape models, and access to expert pri-       107
                                                                                           ors on symmetry or stoichiometry, all of which are hard to         108
                                                                                           guarantee in high-throughput screening or when metastable          109

             Descriptors ⊕                                     Decoder (AR)                phases coexist. Ambiguities accumulate when preferred              110
                                    Conditioning
        None / Comp / Comp+SG                         Tokens     Decoder ×N   Logits       orientation, sample displacement, or low signal-to-noise           111
                                                                                           dramatically alter the profile, motivating approaches that         112
                                                                                           can translate raw PXRD intensities into candidate structures       113
                                  Simulate PXRD                                            without manual feature engineering.                                114
                                     Per CIF

           Rwp Reranking                                       Beam Search                 Learning-based PXRD interpretation. Curated PXRD                   115
                                                                                           corpora (e.g., NOMA, CHILI-100K) have enabled fast dis-            116
                                                                                           criminative tools: BraggNN accelerates Bragg-peak anal-            117
                                                                                           ysis, and neural classifiers infer crystal symmetry directly       118
                                                                                           from powder patterns [7, 16]. These approaches excel at            119
                                                                                           labeling (phases, symmetry) or producing embeddings but            120
                                                                                           stop short of atomistic reconstruction. Assumptions such           121
                                                        Inherited                          as single-phase patterns and fixed broadening also limit ro-       122
                                    Top-k CIFs
                                                        Contributions                      bustness to experimental artifacts and mixtures, motivating        123
                                                                                           generative models.                                                 124
         Figure 1. X TO CIF pipeline: PXRD → encoder (+descriptors) →
         decoder → beam search → PXRD simulation → Rwp reranking                           Generative crystal models. Composition-conditioned                 125
         → Top-k; blue dashed = inherited, red solid = contributions.
                                                                                           generators span diffusion models, normalizing flows, and           126
                                                                                           autoregressive tokenizers [3, 6, 8, 9, 13]. They can enumer-       127
 078       to sweep beam widths through the shared evaluation har-                         ate symmetry-consistent lattices and emit plausible CIFs,          128
 079       ness. Candidates are subsequently ordered by their com-                         but typically require clean descriptors (stoichiometry, lat-       129
 080       puted Rwp using the same utilities, which enables post-                         tice parameters, space group) and curated prompts before           130
 081       hoc filtering of the beam without modifying checkpoints.                        decoding [6, 9]. Without an experimental conditioner, prac-        131
 082       In the descriptor-free regime on NOMA this workflow                             titioners must simulate diffraction for large candidate pools      132
 083       reduces Rwp from 0.23 to 0.18 and raises match rate                             to find PXRD-consistent structures, which is costly and ill-       133
 084       (MR) from 5.6% to 8.1% (+45% relative); with Comp or                            suited to rapid, experiment-in-the-loop workflows [8, 13].         134
 085       Comp+SG, X TO CIF keeps MR near 97.3% while trim-
 086       ming Rwp by ∼ 0.03 (Figure 3).                                                  Diffraction-conditioned generative models. Recent ef-              135
 087     • Evaluation protocol. A unified, reproducible proto-                             forts have started to close this gap by injecting scat-            136
 088       col across three conditioning regimes (No descriptors /                         tering data into the generative loop: DeepStruc couples            137
 089       Comp / Comp+SG) with standardized metrics: Valid-                               pair-distribution functions with graph decoders, conditional       138
 090       ity (syntax/unit-cell parameters/bond sanity), MR (RMSD                         diffusion has been explored on electron diffraction, and           139
 091       ≤ 0.5 Å after symmetry-aware alignment that enforces                           DE CIF ER introduced autoregressive CIF decoding from              140
 092       chemical identity), and Rwp computed on simulated                               PXRD embeddings. However, DE CIF ER depends on a sin-              141
 093       PXRD. Unless stated otherwise, we report aggregated                             gle decoding strategy, lacks an explicit mechanism to com-         142
 094       statistics over generated candidates (best-of-beam ex-                          pare multiple hypotheses against the observed trace, and its       143
 095       tracted via the post-hoc filtering stage plus global medi-                      training recipe remains costly to reproduce at scale. X TO -       144
 096       ans/means). Figure 2 shows how these aggregates evolve                          CIF targets these pain points: we reorganize the training          145
 097       with beam size.                                                                 system for multi-GPU efficiency, decouple CIF synthesis            146
                                                                                           from downstream diffraction-aware ranking, and introduce           147
 098     2. Background and Related Work                                                    a test-time scaling strategy that surfaces a calibrated portfo-    148
 099     PXRD-driven structure solution. Powder diffraction re-                            lio of candidates. This rethinking of both streams of prior        149
 100     mains the most practical probe for crystalline electrodes,                        art, namely classical PXRD analysis and modern generative          150
 101     catalysts, and battery intermediates, yet solving unknown                         modeling, grounds the methodological choices detailed in           151
 102     structures from one-dimensional scattering traces is still                        the next section.                                                  152


                                                                                       2
CVPR                                                                                                                                                           CVPR
#XXXX                                                                                                                                                          #XXXX
                              CVPR 2025 Submission #XXXX. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.



 153    3. Methods                                                           Run                GPUs      Iter (s)    Batch eff.   Throughput (s−1 ) Speedup
                                                                             Reference           1     256.90 ± 7.36 32 × 40 × 1     4.99 ± 0.14      1.0×
 154    3.1. PXRD Conditioning Pipeline                                      Streamlined data    1     29.17 ± 2.84 32 × 40 × 1      44.28 ± 4.51    8.84×
                                                                             Efficient PXRD      1     10.35 ± 0.13 32 × 40 × 1        124 ± 2       24.9×
 155    We inherit the conditioning signal introduced by DE CIF ER.          Distributed         2      5.82 ± 0.19  32 × 20 × 2       220 ± 7       44.3×
 156    Discrete peak sets are broadened directly in reciprocal-
                                                      −1                     Table 1. NOMA throughput (iters 1–10 post warm-up); effective
 157    space: by default we discretize q ∈ [0, 10] Å with ∆q =
                                                                             batch = (per-device)×(accum)×(GPUs); speedup vs reference.
 158    0.01, sample the pseudo-Voigt full-width-at-half-maximum
 159    from the configurable range [0.001, 0.10], and draw the
 160    Gaussian noise standard deviation from [0.001, 0.05]. In-
                                                                             therefore keep the original 8-layer, 8-head GPT-style de-                          204
 161    tensities can be re-scaled and Bernoulli masking can be en-
                                                                             coder with GELU feed-forward blocks and learned absolute                           205
 162    abled, although the released configuration fixes the scale to
                                                                             position embeddings; we retain this backbone intentionally                         206
 163    one and disables masking. The outcome is a single-channel
                                                                             so that X TO CIF remains a drop-in upgrade for existing DE -                       207
 164    trace y(q) that can be converted back to 2θ if desired, but is
                                                                             CIF ER runs.                                                                       208
 165    consumed as-is by the conditioner.
 166        While the statistical model matches the public release,             The optimization objective is the standard cross-entropy                        209

 167    its implementation has been rewritten to stream the pseudo-          of the next CIF token with masked padding positions (no                            210

 168    Voigt computation in GPU-friendly chunks, keeping in-                label smoothing or auxiliary heads are active by default).                         211

 169    termediate tensors within a fixed memory budget. Sam-                We experimented with temperature annealing and prefix-                             212

 170    pling, broadening, and normalization now run in-line with            classification heads, but the public recipe keeps the simpler                      213

 171    data loading so that the decoder always receives on-device           loss and relies on the evaluation harness to discard invalid                       214

 172    PXRD embeddings without incurring host/device round-                 generations. Practitioners can still script additional regular-                    215

 173    trips. These engineering changes make augmentation over-             izers through the released code, yet all reported checkpoints                      216

 174    head negligible compared to the decoder forward pass and             are trained with the vanilla objective described above.                            217

 175    unlock the throughput gains summarized later in Table 1.
                                                                             3.3. Train Optimization                                                            218
 176        Consistent with the public DE CIF ER release, we treat
 177    the resulting diffractogram as a single flattened vector and         Compared with the baseline implementation, the training                            219
 178    map it to the decoder width through a shallow multilayer             pipeline has been streamlined to shorten time-to-result and                        220
 179    perceptron. The MLP (linear →ReLU stack terminating                  improve experimental control:                                                      221
 180    in a projection to 512 dimensions) produces conditioning             • Multi-GPU orchestration. Native distributed training                             222
 181    tokens that are inserted next to the CIF “data ” delimiters             now balances mini-batches across accelerators, averages                         223
 182    during training and inference. Metadata descriptors (Comp               validation statistics synchronously, and delivers near-                         224
 183    / Comp+SG) remain part of the textual prompt: the evalu-                linear throughput gains when additional devices are avail-                      225
 184    ation harness simply preserves the relevant CIF-header text             able.                                                                           226
 185    whenever those hints are available, avoiding extra learned           • Streamlined data serving. The data loaders employ per-                           227
 186    tokens. This design keeps the conditioner lightweight,                  sistent workers, staged host-to-device transfers, and op-                       228
 187    matching the reference implementation in this repository,               tional split subsampling, which together shorten input la-                      229
 188    while still benefiting from the GPU-resident pseudo-Voigt               tency and stabilize gradient updates on large batches.                          230
 189    pipeline above.                                                      • Efficient PXRD augmentation. The pseudo-Voigt trans-                             231
                                                                                formation processes diffraction peaks in GPU-resident                           232
 190    3.2. Autoregressive CIF Decoder                                         chunks, keeping the augmentation stage memory-                                  233
 191    The CIF generator is a decoder-only transformer that shares             bounded while retaining the fidelity of the broadened pro-                      234
 192    its tokenizer and vocabulary with DE CIF ER. CIF strings are            files.                                                                          235
 193    segmented into lattice tokens, atomic species tokens, frac-              At inference time we rely on the same evaluation har-                          236
 194    tional coordinates, occupancy values, and symmetry oper-             ness that mirrors the training stack. Sampling defaults to a                       237
 195    ations. During training we prepend a start-of-sequence to-           single hypothesis, but the decoder can switch to determin-                         238
 196    ken and splice the PXRD conditioning tokens next to ev-              istic beam search with widths up to the range explored in                          239
 197    ery “data ” marker before teacher-forcing the ground-truth           our sweeps. Each candidate is logged once per generation                           240
 198    CIF tokens. Descriptor regimes are handled by keeping                pass, after which an offline stage computes Rwp , RMSD,                            241
 199    (or dropping) the corresponding textual metadata inside the          and validity summaries and can optionally retain only the                          242
 200    prompt; we do not add separate learned descriptor em-                top-ranked hypotheses per CIF. Separating reranking from                           243
 201    beddings. Boundary-masking prevents conditioning tokens              decoding keeps the generation loop lightweight while still                         244
 202    from attending across CIF instances when multiple samples            exposing the deterministic beam analyses used throughout                           245
 203    are packed into the same context. The released checkpoints           the paper.                                                                         246


                                                                         3
CVPR                                                                                                                                                                       CVPR
#XXXX                                                                                                                                                                      #XXXX
                             CVPR 2025 Submission #XXXX. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.



                                                                                                                          0.08
         Algorithm 1: Deterministic beam search with of-                                            0.080          0.08              0.08
                                                                                                                                                                    0.08

         fline diffraction-aware reranking                                                          0.075
           Input: PXRD y, optional descriptors d, beam width B,




                                                                                  Rmsd Match Rate
                                                                                                    0.070
                    length exponent α
           Output: ranked pool C, best s⋆ , median s̃                                               0.065
           z ← conditioning prefix built from (y, d)                                                0.060
           H ← deterministic beam search on z with width B and                                                 0.06                         Baseline evaluation
                                                                                                    0.055                                   (temperature/top-k sampling)
             length control α (no sampling)                                                                 0.05                            Beam search
                                                                                                                                            B=1 baseline
           Serialize H and evaluate each hypothesis offline                                         0.050
                                                                                                             0 1 2         5          10                             20
           Prune invalid CIFs and merge symmetry-equivalent                                                                      Beam size (B)
             hypotheses
           C ← keep the B best log-likelihood sequences from H              Figure 2. Beam size B vs RMSD match rate on NOMA (RMSD ≤
           for s ∈ C do                                                     0.5 Å): gains increase to B = 5 then saturate; baseline is greedy
                I calc ← PXRD simulated from s                              (B = 1).
                rs ← Rwp y, I calc
                                    

           Order C by rs (break ties with length-normalized
             log-likelihood)                                                RMSD ≤ 0.5 Å; and (iii) the weighted-profile residual                                          276
           s⋆ ← first element of C                                          Rwp calculated on the simulated PXRD. Unless noted, best-                                       277
           s̃ ← median element of C                                         of-beam statistics refer to the offline filtering stage de-                                     278
           return (C, s⋆ , s̃)                                              scribed above, which ranks hypotheses by increasing Rwp                                         279
                                                                            (ties resolved by length-normalized log-probability). We re-                                    280
                                                                            port global medians/means over the retained candidates to                                       281
                                                                            assess robustness.                                                                              282
 247    3.4. Decoding and Reranking
 248    When we study deterministic decoding we disable sampling            Beam search. We decode with a beam of width B to re-                                            283
 249    and rely on the beam-search mode built into the decoder.            tain multiple high-probability hypotheses and reduce sam-                                       284
 250    Algorithm 1 summarizes the scoring procedure used in our            pling variance. To limit length bias, we normalize scores                                       285
 251    analyses. In practice we first materialize all beam outputs         as                                                                                              286
 252    and serialize them; the filtering and Rwp -based ordering                                     1 X
                                                                                                          L
 253    happen offline inside the evaluation pipeline. Leaving the                    score(y1:L ) = α       log P (yt | y<t ),    (1)                                      287
                                                                                                     L t=1
 254    heavy reranking step in post-processing keeps the genera-
 255    tion loop lightweight while faithfully implementing the de-         We set α = 1.0 for all reported results and forward the                                         288
 256    terministic portfolio analysis discussed below.                     resulting portfolio to diffraction-aware filtering.                                             289


 257    Portfolio construction and deduplication. When deter-               Rwp Ranking: Once the decoding stage has produced a                                             290
 258    ministic mode is enabled we maintain a beam of width B to           portfolio of CIF candidates, we move beyond token-level                                         291
 259    explore multiple high-probability continuations. Sequences          likelihoods and examine how well each structure captures                                        292
 260    are scored with length-normalized log-likelihood (Eq. (1)),         the underlying crystallography. Every candidate is con-                                         293
 261    and we prune on end-of-sequence. The collection script              verted back into atomic coordinates and symmetry infor-                                         294
 262    discards syntactically invalid CIFs, deduplicates symmetry-         mation, after which we compute descriptors that summarize                                       295
 263    equivalent hypotheses (tested via the symmetry operators            the agreement with the experimental diffraction signal. This                                    296
 264    present in the decoded CIF header), and forwards every sur-         diffraction-aware ordering prevents the decoder from priv-                                      297
 265    viving hypothesis (at most B per CIF, though practition-            ileging the sequence that the language model deems most                                         298
 266    ers can raise B when compute allows) to diffraction-aware           probable when that choice disagrees with the PXRD evi-                                          299
 267    reranking.                                                          dence. We therefore rank candidates using the weighted-                                         300
                                                                            profile residual Rwp . For each generated CIF, we simulate                                      301
 268    3.5. Evaluation Protocol                                            the corresponding powder diffraction pattern and compare                                        302
                                                                            it to the experimental profile using the standard refinement                                    303
 269    We evaluate each test CIF under the same forward model
                                                                            residual                                                                                        304
 270    used for conditioning. For every generated candidate we
                                                                                                                           1/2
 271    compute: (i) validity, which requires a parsable CIF, a                                            obs
                                                                                                               − Ikcalc )2
                                                                                              P
                                                                                                   k wk (Ik
 272    positive-definite unit cell with reasonable edges/angles, oc-                  Rwp =         P         obs 2
                                                                                                                                ,                                           305
                                                                                                        k wk (Ik )
 273    cupancies in (0, 1], and plausible bonds; (ii) the RMSD
 274    match rate (MR), i.e., the fraction of samples whose                where Ikobs and Ikcalc denote the observed and calculated in-                                   306
 275    symmetry-aware alignment respecting composition attains             tensities at sampling point k, and wk is the customary statis-                                  307


                                                                        4
CVPR                                                                                                                                                                    CVPR
#XXXX                                                                                                                                                                   #XXXX
                                          CVPR 2025 Submission #XXXX. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.



        Figure 3. Decoding refinements on NOMA (B=5, N=1,000): Rwp                                   FWHM from [0.001, 0.10], and draw Gaussian noise from               345
        (µ ± σ)↓ and MR↑; largest gains in No descriptors, smaller in                                [0.001, 0.05]. The released configuration keeps the inten-          346
        Comp/Comp+SG.                                                                                sity scale at one and disables Bernoulli masking, but both          347
                                                                                                     options are exposed for experiments that require them. For          348
        Descriptor       Decoding stack                               Rwp (µ ± σ) ↓   MR (%) ↑
                         deCIFer (decode)                              0.23 ±0.26       5.61
                                                                                                     inference we regenerate the PXRD trace from the held-out            349
        No descriptors   deCIFer (decode + beam search)                0.18 ±0.22       7.71         CIF using the same simulator so that comparisons are not            350
                         deCIFer (decode + beam search + Rwp rank.)    0.18 ±0.22       8.11
                         deCIFer (decode)                              0.18 ±0.21      96.28
                                                                                                     confounded by mismatched instrumental kernels. Exper-               351
        Comp             deCIFer (decode + beam search)                0.15 ±0.19      97.29         iments are run under three conditioning regimes: No de-             352
                         deCIFer (decode + beam search + Rwp rank.)    0.14 ±0.19      97.29
                         deCIFer (decode)                              0.17 ±0.21      96.79
                                                                                                     scriptors (PXRD only), Comp (PXRD plus a stoichiomet-               353
        Comp+SG          deCIFer (decode + beam search)                0.15 ±0.19      97.30         ric hint), and Comp+SG (PXRD plus stoichiometry and                 354
                         deCIFer (decode + beam search + Rwp rank.)    0.15 ±0.19      97.30
                                                                                                     Hermann–Mauguin symbol). Descriptor information is pre-             355
                                                                                                     served by keeping the relevant text in the prompt through           356
                                                                                                     the evaluation harness, so no additional learned tokens are         357
 308    tical weight. A small Rwp signals that the simulated diffrac-                                introduced. These regimes reveal how X TO CIF behaves as            358
 309    tion curve closely follows the measured pattern, implying                                    metadata availability ranges from absent to highly informa-         359
 310    that the candidate structure reproduces the long-range order                                 tive.                                                               360
 311    encoded in the experiment. By ordering the beam outputs
 312    from lowest to highest Rwp , we identify CIFs that are not                                   4.3. Training and Inference Configuration                           361

 313    only geometrically consistent but also compatible with the                                   Unless otherwise noted, models are trained for 300 k op-            362
 314    diffraction evidence, leading to samples that are ready for                                  timizer steps with batch size 32×40 gradient accumulation           363
 315    subsequent refinement.                                                                       on two NVIDIA A100 GPUs (the released configuration ex-             364
 316       All ablation studies in this section therefore focus exclu-                               poses shorter or longer schedules through a single iteration-       365
 317    sively on DE CIF ER, leaving other conditioned variants to                                   count knob). We reuse the optimizer hyperparameters listed          366
 318    future work.                                                                                 in Section 3.4 and apply teacher-forcing on randomly sam-           367
                                                                                                     pled augmentation levels. Validation snapshots are taken            368
 319    4. Dataset and Experiments                                                                   every 2 k steps using the in-distribution augmentation pro-         369
                                                                                                     file.                                                               370
 320    4.1. Benchmarks and Splits                                                                       At test time we run the same evaluation harness. The de-        371
 321    All quantitative results in this paper are obtained on                                       fault configuration samples a single hypothesis (B = 1)             372
 322    NOMA, which aggregates relaxed structures from NO-                                           with temperature 1.0. Deterministic beam search up to               373
 323    MAD, OQMD, and Materials Project snapshots (April 2023                                       B = 20 is available, and automation handles the beam-               374
 324    cut-off) and supplies synthetic PXRD traces computed with                                    width sweeps reported later in the paper. After generation          375
 325    a fixed instrumental model [1, 4, 12]. After deduplicating                                   we compute Rwp and RMSD for every candidate and op-                 376
 326    by reduced formula and removing entries with unresolved                                      tionally retain the top-k hypotheses per CIF via a post-hoc         377
 327    oxidation states, the working set contains 2.1 M CIFs. We                                    filtering stage. All best-of-beam statistics reported in the pa-    378
 328    stratify this pool by Bravais class and element count, reserv-                               per refer to this filtering step, and medians/means are com-        379
 329    ing 2.0 M structures for training, 50 k for validation, and                                  puted over the resulting candidate sets for each evaluation         380
 330    20 k for testing. We additionally maintain data-processing                                   run.                                                                381
 331    scripts for CHILI-100K [11, 15], the experimental col-
 332    lection of 100 k powder scans gathered with a laboratory                                     4.4. Evaluation Metrics                                             382
 333    diffractometer. These utilities regenerate CHILI traces with                                 We report three complementary metrics on NOMA-test: (1)             383
 334    the same inference pipeline used on NOMA, producing a                                        Validity, the fraction of CIFs that pass syntax checks, yield       384
 335    descriptor-free pilot evaluation on 841 held-out scans (Ta-                                  a stable lattice, and contain plausible bonds; (2) Match            385
 336    ble 2). Both corpora share the 373-token vocabulary in-                                      Rate (MR), the percentage of samples whose symmetry-                386
 337    herited from DE CIF ER, ensuring backwards compatibility                                     aligned RMSD falls below 0.5 Å while respecting com-               387
 338    whenever we alternate between synthetic and experimental                                     position; and (3) the weighted-profile residual Rwp , ob-           388
 339    diffraction.                                                                                 tained by comparing the simulated PXRD of each candidate            389

 340    4.2. PXRD Synthesis and Conditioning Regimes                                                 against the reference trace. When the conditioning regime           390
                                                                                                     provides no descriptors, we still enforce exact atomic iden-        391
 341    To emulate realistic diffraction variability, every CIF is                                   tities to avoid artificially inflating MR. All metrics are re-      392
 342    passed through the reciprocal-space simulator described in                                   ported as mean ± standard deviation over the retained can-          393
 343    Section 4: by default we evaluate the pseudo-Voigt pro-                                      didates from a given evaluation run; whenever we mention            394
                                −1
 344    file on a q ∈ [0, 10] Å grid with ∆q = 0.01, sample                                         “best-of-beam,” we refer to the post-hoc filtering step de-         395


                                                                                                 5
CVPR                                                                                                                                                           CVPR
#XXXX                                                                                                                                                          #XXXX
                              CVPR 2025 Submission #XXXX. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.



                                                                                                                          100%
 396    scribed above.                                                                                                                   100
                                                                                                  83 (86%)
                                                                                            80                                           80
 397    4.5. Experiment Suite




                                                                                                                                              Cumulative (%)
                                                                             Count (N=96)
                                                                                            60                                           60
 398    The main study contrasts X TO CIF with the original DE -
                                                                                            40                                           40
 399    CIF ER under all conditioning regimes on NOMA-test,
                                                                                            20                                           20
 400    highlighting the impact of the streamlined training loop and                                                     13 (14%)

 401    the decoding/reranking stack. A second set of experiments                            0
                                                                                                 Compositio             Atom count
                                                                                                                                         0
                                                                                                 n mismatch              mismatch
 402    sweeps beam size and the number of reranked candidates to
 403    map out the test-time scaling frontier (Figure 2). Finally, we       Figure 4. Non-match causes on NOMA without conditioning
 404    ablate each ingredient: conditioning augmentations, beam             (PXRD-only, N=96 failures from the legacy DE CIF ER evaluation
 405    width, and Rwp -based filtering, to quantify its contribution        logs): composition mismatch dominates; bars are counts, the line
 406    to MR and Rwp in the descriptor-free regime, which is the            is cumulative %, so stoichiometry fixes remain the clearest lever
 407    most challenging real-world setting. The same infrastruc-            to raise MR.
 408    ture now powers the pilot CHILI-100K evaluation summa-
 409    rized in Section 5.4.
                                                                             cells with bounded edges/angles, occupancies in (0, 1],                            444
 410    5. Results                                                           bond-length sanity checks) and from the per-CIF dedupli-                           445
                                                                             cation implied by the best-of-beam filter. Section 5.4 reuses                      446
 411    5.1. Faster Training Enables Wider Sweeps                            the exact same protocol.                                                           447

 412    Table 1 shows that the optimized pipeline cuts single-GPU
 413    iteration time from 258 ± 4 s to 10.3 ± 0.1 s and scales             Why do matches fail? Inspecting the descriptor-free                                448
 414    to 5.8 ± 0.2 s on two GPUs, a 44× throughput gain rela-              evaluation logs with the released Pareto script (Fig. 4)                           449
 415    tive to the public DE CIF ER recipe. This shrinks a 300 k-           shows that most of the 96 RMSD failures on the 1,000-                              450
 416    step run from ∼ 10 days to ∼ 15 hours and makes system-              sample slice arise from composition inconsistencies, fol-                          451
 417    atic restart sweeps practical. Replaying the released evalua-        lowed by atom-count mismatches; purely geometric er-                               452
 418    tion sweep on 1,000 NOMA samples with composition de-                rors are rare once stoichiometry matches. This points to                           453
 419    scriptors keeps the baseline greedy sampler at 96.3 % MR             lightweight composition priors, such as classifier heads,                          454
 420    (matching the public checkpoint) but lifts MR to 97.3 %              two-stage stoichiometry proposals, or PXRD-derived hints,                          455
 421    once we enumerate B=5 beams and re-rank them, all with-              as the clearest lever to raise MR in metadata-free deploy-                         456
 422    out altering the architecture, tokenizer, or loss.                   ments.                                                                             457

 423    5.2. Accuracy on NOMA                                                5.3. Test-Time Scaling Effects                                                     458

 424    In the descriptor-free regime, deterministic beam search             Figure 2 highlights the match-rate gains we obtain as B                            459
 425    with B = 5 coupled with the offline Rwp ranking stage                grows. Improvements are steep up to B = 5, where MR                                460
 426    (Fig. 3) improves both diffraction fidelity and geometry.            almost doubles relative to greedy decoding. Beyond that                            461
 427    The released runs reduce Rwp from 0.23 ± 0.26 to 0.18 ±              point most hypotheses are symmetry-equivalent, yet the                             462
 428    0.22 and raise MR from 5.6 % to 8.1 %, yielding a +2.5               extra samples still help the offline Rwp filter rescue low-                        463
 429    pp absolute (+45 %) relative gain, purely through decod-             probability but diffraction-consistent candidates. The scor-                       464
 430    ing and reranking. When composition or composition-plus-             ing function in Eq. 1 uses α = 1.0; modest perturbations of                        465
 431    space-group metadata are carried through the prompts, the            α or B > 20 produce marginal differences, so we default to                         466
 432    same sweep keeps MR fixed at 97.3 % while trimming Rwp               B = 5 and only fan out further when coverage is critical.                          467
 433    by 0.03 (Comp) and 0.02 (Comp+SG), nudging these sat-
 434    urated configurations toward the 98 % coverage band that             5.4. First Look at CHILI-100K                                                      468
 435    practitioners target. Qualitatively, the best-of-beam sur-           To gauge how far the NOMA-trained checkpoints trans-                               469
 436    vivors capture symmetry-related variants that the greedy             fer, we replayed the descriptor-free evaluation pipeline on                        470
 437    sampler would have discarded.                                        841 CHILI-100K scans using the same B=5 deterministic                              471
                                                                             beam search and reranking configuration as on NOMA. Ta-                            472
 438    Validity (vs. DE CIF ER). Across NOMA regimes, valid-                ble 2 contrasts these results with the corresponding syn-                          473
 439    ity stays within ±1 pp of the public baseline: descriptor-free       thetic sweeps.                                                                     474
 440    runs move from 85.6 % (greedy) to 86.9 % (beam+Rwp );                    Three trends emerge. First, syntactic validity only drops                      475
 441    composition-aware ones sit at ∼ 97 %. Apparent drops rel-            from 86.9 % on NOMA to 67.9 % on CHILI, confirm-                                   476
 442    ative to prior reports stem from the stricter evaluation we          ing that the tokenizer and decoder remain stable on ex-                            477
 443    apply to all methods (successful parsing, positive-definite          perimental scans even under the stricter validator. Sec-                           478


                                                                         6
CVPR                                                                                                                                                                            CVPR
#XXXX                                                                                                                                                                           #XXXX
                                        CVPR 2025 Submission #XXXX. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.



        Table 2. Descriptor-free decoding on CHILI-100K vs. NOMA.                                            maining gap to experimental corpora such as CHILI-100K              519
        The experimental CHILI scans sharply reduce RMSD matches de-                                         will require measured instrument response functions or dif-         520
        spite similar validity, while Rwp remains higher because the traces                                  ferentiable forward models that adapt to per-scan idiosyn-          521
        include unmodelled instrument effects.
                                                                                                             crasies, an avenue we leave for future work.                        522
        Dataset      Decoding stack                               Valid (%)   Rwp (µ ± σ) ↓   MR (%) ↑
                     deCIFer (decode)                               85.6       0.23 ± 0.26      5.6
        NOMA         deCIFer (decode + beam search)
                     deCIFer (decode + beam search + Rwp rank.)
                                                                    86.5
                                                                    86.9
                                                                               0.18 ± 0.22
                                                                               0.18 ± 0.22
                                                                                                7.7
                                                                                                8.1
                                                                                                             Scaling inference responsibly. Test-time scaling via                523
                     deCIFer (decode)                               65.8       0.86 ± 0.35      0.23         beam search and Rwp reranking delivers clear benefits, but          524
        CHILI-100K   deCIFer (decode + beam search)                 68.1       0.81 ± 0.35      0.36
                     deCIFer (decode + beam search + Rwp rank.)     67.9       0.81 ± 0.35      0.36
                                                                                                             it also increases compute and memory consumption linearly           525
                                                                                                             with beam width. In laboratory settings where dozens of             526
                                                                                                             unknowns must be solved per day, we must balance accu-              527
 479    ond, the RMSD match rate collapses below 0.4 % (and the                                              racy with cost. Adaptive beam allocation, where we grow             528
 480    space-group match rate from 92 % to 1 %) because peak                                                the beam only when the PXRD evidence is ambiguous, or               529
 481    shifts and impurities violate the symmetry-aware alignment                                           early-exit criteria based on intermediate Rwp signals could         530
 482    used for reference CIFs. Third, mean Rwp rises from 0.18                                             keep inference budgets manageable without sacrificing fi-           531
 483    to 0.81, indicating that the synthetic instrument underes-                                           delity. Because the descriptor-free regime saw the largest          532
 484    timates background, preferred orientation, and secondary                                             relative gain (Figure 3), we view it as the most impact-            533
 485    phases present in CHILI diffractograms. Even so, beam                                                ful setting for future research on adaptive beams and cost-         534
 486    restarts continue to help: reranking slightly improves Rwp                                           aware reranking.                                                    535
 487    despite the near-absence of RMSD matches.
 488       Closing this domain gap will require conditioning the
                                                                                                             Generalization limits and future data. Evaluating on                536
 489    simulator on the CHILI optics, mixing experimental scans
                                                                                                             experimental PXRD remains an open milestone. Low-                   537
 490    during fine-tuning, and/or augmenting the scoring pipeline
                                                                                                             symmetry, multi-element oxides with broad peaks dominate            538
 491    with peak-level alignment that tolerates preferred orienta-
                                                                                                             emerging energy materials, so future work should curate             539
 492    tion. Those upgrades are now engineering tasks rather
                                                                                                             additional experimental corpora (e.g., CHILI-100K) with             540
 493    than missing infrastructure: the same evaluation scripts,
                                                                                                             richer metadata such as instrument geometry, sample prepa-          541
 494    prompts, and checkpoints used on NOMA already operate
                                                                                                             ration notes, and phase-fraction estimates. Mixing such cor-        542
 495    end-to-end on CHILI.
                                                                                                             pora into training, possibly via curriculum schedules that          543
 496    5.5. Ablations                                                                                       anneal from clean to noisy traces, could tighten the remain-        544
                                                                                                             ing gap between simulated and measured PXRD. Given that             545
 497    Removing the PXRD augmentation schedule degrades                                                     descriptor-free PXRD is ubiquitous yet difficult, we rec-           546
 498    descriptor-free match rate by 1.6 % absolute, primarily be-                                          ommend prioritizing data collection and modeling advances           547
 499    cause the model overfits to narrow peaks and fails when the                                          that operate without chemical hints before chasing marginal         548
 500    test trace exhibits instrumental broadening. Finally, skip-                                          gains in already-saturated conditioned settings.                    549
 501    ping the post-hoc Rwp filtering while keeping beam search
 502    preserves the validity gains yet raises the best-of-beam Rwp
 503    by ∼ 0.03, underscoring that token likelihood alone is a                                             User-facing workflows. Ultimately the value of X TO CIF             550
 504    poor proxy for diffraction consistency. Collectively, these                                          depends on seamless integration into refinement suites. Our         551
 505    studies justify the methodological choices outlined in Sec-                                          current prototype emits a ranked list of CIFs; crystallogra-        552
 506    tions 3.4 and 4.                                                                                     phers still need to inspect and, if necessary, refine these can-    553
                                                                                                             didates manually. Integrating with existing Rietveld soft-          554
 507    6. Discussion and Outlook                                                                            ware via standardized APIs or generating scripts that repro-        555
                                                                                                             duce the top candidates in GSAS-II/FullProf would accel-            556
 508    Bridging simulation and experiment. DE CIF ER first                                                  erate adoption. Likewise, exposing uncertainty estimates            557
 509    demonstrated that PXRD-conditioned autoregressive de-                                                (e.g., via ensemble disagreement across beams) would help           558
 510    coding over CIF tokens can close the loop between diffrac-                                           practitioners judge when automated suggestions are trust-           559
 511    tion traces and atomistic structures. X TO CIF inherits this                                         worthy.                                                             560
 512    interface but strengthens the practical story: faster train-
 513    ing makes descriptor-free checkpoints easier to refresh, and
 514    the deterministic beam plus Rwp reranker reduces failure                                             Broader implications. Beyond PXRD, the conditioning                 561
 515    cases when no chemical metadata are available. The reduc-                                            pipeline is agnostic to the specific diffraction modality;          562
 516    tion in Rwp for descriptor-free inference suggests that lan-                                         electron and neutron scattering profiles could be embedded          563
 517    guage models can internalize peak-to-structure mappings                                              with minimal changes, enabling multi-modal structure in-            564
 518    traditionally handled by iterative refinement. Closing the re-                                       ference. Furthermore, the autoregressive framework lends            565


                                                                                                         7
CVPR                                                                                                                                                    CVPR
#XXXX                                                                                                                                                   #XXXX
                                CVPR 2025 Submission #XXXX. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.



 566    itself to active-learning loops in autonomous labs: by simu-                    data using autoregressive language models. arXiv preprint        618
 567    lating PXRD under hypothetical synthesis routes and rank-                       arXiv:2502.02189, 2025. 1                                        619
 568    ing the most informative measurements, the same model                       [6] L. M. Antunes, K. T. Butler, and R. Grau-Crespo. Crystal         620
 569    could inform experiment scheduling. We expect these di-                         structure generation with autoregressive large language mod-     621
 570    rections to shape the next milestones on the path toward                        eling. Nature Communications, 15 (1): 10570, 2024. ISSN          622
 571    reliable, fully automated structure solution for complex ma-                    2041-1723. doi: 10.1038/s41467-024-54639-7. 2                    623
 572    terials workflows.                                                          [7] Ziming Liu, Himanshu Sharma, Jun-Sang Park, Peter Ke-            624
                                                                                        nesei, Antonino Miceli, Jonathan Almer, Rajkumar Ket-            625
                                                                                        timuthu, and Ian Foster. BraggNN: fast x-ray bragg peak          626
 573    7. Conclusion
                                                                                        analysis using deep learning. IUCrJ, 8(6):1140–1153, 2021.       627
 574    DE CIF ER established that PXRD-to-structure inference can                      2                                                                628
 575    be cast as a language-modeling problem over CIF to-                         [8] N. Gruver, A. Sriram, A. Madotto, A. G. Wilson, C. L. Zit-       629
 576    kens. X TO CIF keeps that foundation and contributes effi-                      nick, and Z. W. Ulissi. Fine-tuned language models gen-          630
 577    cient training plus diffraction-aware decoding that make the                    erate stable inorganic materials as text. In The Twelfth In-     631
                                                                                        ternational Conference on Learning Representations (ICLR),       632
 578    workflow practical. By streamlining the PXRD preprocess-
                                                                                        2024. 2                                                          633
 579    ing stack and tightening the decoding loop with beam search
                                                                                    [9] R. Jiao, W. Huang, P. Lin, J. Han, P. Chen, Y. Lu, and Y.        634
 580    plus Rwp reranking, we lower Rwp and raise match rate
                                                                                        Liu. Crystal structure prediction by joint equivariant diffu-    635
 581    simultaneously on the challenging descriptor-free NOMA                          sion. Advances in Neural Information Processing Systems,         636
 582    setting. The resulting system trains an order of magni-                         36: 17464–17497, 2023. 2                                         637
 583    tude faster than the public DE CIF ER release, enabling thor-              [10] Juan Rodrı́guez-Carvajal. Recent advances in magnetic            638
 584    ough sweeps that were previously impractical and yielding                       structure determination by neutron powder diffraction. Phys-     639
 585    a more reproducible recipe for follow-up work.                                  ica B: Condensed Matter, 192(1-2):55–69, 1993. FullProf          640
 586       Looking ahead, we expect the same recipe to transfer to                      Suite. 1, 2                                                      641
 587    experimental corpora such as CHILI-100K once the data                      [11] S. Gražulis, D. Chateigner, R. T. Downs, A. F. T. Yokochi,      642
 588    collection campaign is complete; bridging that gap will re-                     M. Quirós, L. Lutterotti, E. Manakova, J. Butkus, P. Moeck,     643
 589    quire richer augmentation, adaptive beam allocation, and                        and A. Le Bail. Crystallography Open Database – an               644
 590    tighter integration with refinement software.                                   open-access collection of crystal structures. Journal of Ap-     645
 591       Taken together, these advances demonstrate that test-                        plied Crystallography, 42 (4): 726–729, Aug 2009. doi:           646
                                                                                        10.1107/S0021889809016690. 5                                     647
 592    time scaling and diffraction-aware ranking can elevate
                                                                                   [12] S. Kirklin, J. E. Saal, B. Meredig, A. Thompson, J. W. Doak,     648
 593    autoregressive CIF generators from proof-of-concept to
                                                                                        M. Aykol, S. Rühl, and C. Wolverton. The open quantum           649
 594    deployment-ready tooling. We expect the techniques pre-
                                                                                        materials database (oqmd): assessing the accuracy of dft for-    650
 595    sented here, particularly the training throughput improve-                      mation energies. npj Computational Materials, 1 (1): 1–15,       651
 596    ments and inference stack, to inform subsequent work and                        2015. 5                                                          652
 597    to accelerate the broader adoption of PXRD-driven genera-                  [13] T. Mohanty, M. Mehta, H. M. Sayeed, V. Srikumar, and T.          653
 598    tive models in automated materials discovery workflows.                         D. Sparks. Crystext: A generative ai approach for text-          654
                                                                                        conditioned crystal structure generation using llm. Chem-        655
 599    References                                                                      Rxiv, 2024. doi: 10.26434/chemrxiv-2024-gjhpq. This con-         656
                                                                                        tent is a preprint and has not been peer-reviewed. 2             657
 600     [1] A. Jain, S. P. Ong, G. Hautier, W. Chen, W. D. Richards,
                                                                                   [14] Brian H. Toby and Robert B. Von Dreele. GSAS-II: the gene-       658
 601         S. Dacek, S. Cholia, D. Gunter, D. Skinner, G. Ceder, and K.
                                                                                        sis of a modern open-source all purpose crystallography soft-    659
 602         A. Persson. Commentary: The materials project: A materials
                                                                                        ware package. Journal of Applied Crystallography, 46(2):         660
 603         genome approach to accelerating materials innovation. APL
                                                                                        544–549, 2013. 1, 2                                              661
 604         Materials, 1 (1): 011002, 07 2013. ISSN 2166-532X. doi:
 605         10.1063/1.4812323. 5                                                  [15] U. Friis-Jensen, F. L. Johansen, A. S. Anker, E. B. Dam,         662
 606     [2] A. K. Cheetham and A. L. Goodwin. Crystallography with                     K. M. O. Jensen, and R. Selvan. Chili: Chemically-               663
 607         powders. Nature Materials, 13 (8): 760–762, Aug 2014. 1                    informed large-scale inorganic nanomaterials dataset for ad-     664
                                                                                        vancing graph machine learning. In Proceedings of the 30th       665
 608     [3] B. K. Miller, R. T. Chen, A. Sriram, and B. M. Wood.
                                                                                        ACM SIGKDD Conference on Knowledge Discovery and                 666
 609         Flowmm: Generating materials with riemannian flow match-
                                                                                        Data Mining, KDD ’24, page 4962–4973, New York, NY,              667
 610         ing. In Forty-first International Conference on Machine
                                                                                        USA, 2024. Association for Computing Machinery. ISBN             668
 611         Learning, 2024. 2
                                                                                        9798400704901. doi: 10.1145/3637528.3671538. 5                   669
 612     [4] C. Draxl and M. Scheffler. The nomad laboratory: From data
 613         sharing to artificial intelligence. Journal of Physics: Materi-       [16] P. M. Vecsei, K. Choo, J. Chang, and T. Neupert. Neural          670
 614         als, 2, 05 2019. doi: 10.1088/2515-7639/ab13bb. 5                          network based classification of crystal symmetries from x-       671
 615     [5] F. Lizak Johansen and U. Friis-Jensen and E. B. Dam and                    ray diffraction patterns. Physical Review B, 99(24):245120,      672
 616         K. M. Ørnsbjerg Jensen and R. Mercado and R. Selvan. de-                   2019. 2                                                          673
 617         cifer: Crystal structure prediction from powder diffraction


                                                                               8
